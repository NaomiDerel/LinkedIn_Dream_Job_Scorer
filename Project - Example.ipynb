{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe396b7-702b-4ad9-80b4-45ce420f2922",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example of Our Product In Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f57282f-889f-42ae-8a45-79259245be1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: google.generativeai in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (0.5.0)\nRequirement already satisfied: protobuf in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google.generativeai) (4.25.3)\nRequirement already satisfied: google-ai-generativelanguage==0.6.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google.generativeai) (0.6.1)\nRequirement already satisfied: pydantic in /databricks/python3/lib/python3.9/site-packages (from google.generativeai) (2.6.4)\nRequirement already satisfied: google-api-python-client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google.generativeai) (2.125.0)\nRequirement already satisfied: google-auth>=2.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google.generativeai) (2.29.0)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.9/site-packages (from google.generativeai) (4.11.0)\nRequirement already satisfied: google-api-core in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google.generativeai) (2.18.0)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.9/site-packages (from google.generativeai) (4.66.2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-ai-generativelanguage==0.6.1->google.generativeai) (1.23.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-api-core->google.generativeai) (1.63.0)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /databricks/python3/lib/python3.9/site-packages (from google-api-core->google.generativeai) (2.26.0)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-api-core->google.generativeai) (1.62.1)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-api-core->google.generativeai) (1.62.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-auth>=2.15.0->google.generativeai) (4.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-auth>=2.15.0->google.generativeai) (5.3.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-auth>=2.15.0->google.generativeai) (0.4.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai) (0.6.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2021.10.8)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-api-python-client->google.generativeai) (0.22.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-api-python-client->google.generativeai) (4.1.1)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from google-api-python-client->google.generativeai) (0.2.0)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /databricks/python3/lib/python3.9/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google.generativeai) (3.0.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /databricks/python3/lib/python3.9/site-packages (from pydantic->google.generativeai) (0.6.0)\nRequirement already satisfied: pydantic-core==2.16.3 in /databricks/python3/lib/python3.9/site-packages (from pydantic->google.generativeai) (2.16.3)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting transformers\n  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.9/site-packages (from transformers) (21.0)\nCollecting pyyaml>=5.1\n  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\nCollecting huggingface-hub<1.0,>=0.19.3\n  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.8.0)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.9/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.9/site-packages (from transformers) (2023.12.25)\nCollecting tokenizers<0.19,>=0.14\n  Downloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\nCollecting safetensors>=0.4.1\n  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.9/site-packages (from transformers) (4.66.2)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\nCollecting fsspec>=2023.5.0\n  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\nRequirement already satisfied: pyparsing>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\nInstalling collected packages: pyyaml, fsspec, huggingface-hub, tokenizers, safetensors, transformers\nSuccessfully installed fsspec-2024.3.1 huggingface-hub-0.22.2 pyyaml-6.0.1 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.3\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting torch\n  Downloading torch-2.2.2-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\nCollecting nvidia-cudnn-cu12==8.9.2.26\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.9/site-packages (from torch) (2.11.3)\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\nRequirement already satisfied: typing-extensions>=4.8.0 in /databricks/python3/lib/python3.9/site-packages (from torch) (4.11.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.8.0)\nRequirement already satisfied: fsspec in /local_disk0/.ephemeral_nfs/envs/pythonEnv-987ed29e-5afe-4a27-8fbd-c967c8517e43/lib/python3.9/site-packages (from torch) (2024.3.1)\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\nCollecting nvidia-nccl-cu12==2.19.3\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\nCollecting sympy\n  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\nCollecting triton==2.2.0\n  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\nCollecting networkx\n  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\nCollecting mpmath>=0.19\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-cusparse-cu12, nvidia-cublas-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, networkx, torch\nSuccessfully installed mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.2.2 triton-2.2.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install google.generativeai\n",
    "%pip install transformers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd63295a-38c6-4c69-9be6-86d8853371bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import rand\n",
    "import math\n",
    "import numpy as np\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562e8d5e-ec5f-4544-91dc-5924ce0c7376",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Necessary DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9537b8d-8d39-48d0-9814-82fe57137d1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load companies' features & ranked similarities \n",
    "company_features = spark.read.parquet('/FileStore/shared_uploads/naomi.derel@campus.technion.ac.il/company_features_similarities_small.parquet', header=True)\n",
    "\n",
    "# Load user features\n",
    "user_feature_vectors = spark.read.parquet('/FileStore/shared_uploads/naomi.derel@campus.technion.ac.il/user_feature_vectors.parquet', header=True)\n",
    "\n",
    "# Load companies with features and jobs\n",
    "company_features_jobs = spark.read.parquet(\"/FileStore/shared_uploads/naomi.derel@campus.technion.ac.il/company_with_job_features.parquet\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f13df854-cdb2-48a1-849b-6591676cbf7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " ordered_features = ['years of education',\n",
    " 'years of experience',\n",
    " 'recommendations',\n",
    " 'volunteer experience',\n",
    " 'english',\n",
    " 'spanish',\n",
    " 'chinese',\n",
    " 'tagalog',\n",
    " 'vietnamese',\n",
    " 'french',\n",
    " 'korean',\n",
    " 'german',\n",
    " 'arabic',\n",
    " 'russian',\n",
    " 'italian',\n",
    " 'portuguese',\n",
    " 'polish',\n",
    " 'hindi',\n",
    " 'japanese',\n",
    " 'urdu',\n",
    " 'gujarati',\n",
    " 'persian',\n",
    " 'telugu',\n",
    " 'tamil',\n",
    " 'greek',\n",
    " 'armenian',\n",
    " 'haitian creole',\n",
    " 'filipino',\n",
    " 'bengali',\n",
    " 'panjabi',\n",
    " 'kannada',\n",
    " 'malayalam',\n",
    " 'marathi',\n",
    " 'odia',\n",
    " 'sindhi',\n",
    " 'sinhala',\n",
    " 'assamese',\n",
    " 'nepali',\n",
    " 'haryanvi',\n",
    " 'rajasthani',\n",
    " 'chhattisgarhi',\n",
    " 'maithili',\n",
    " 'bhojpuri',\n",
    " 'magahi',\n",
    " 'kashmiri',\n",
    " 'hebrew',\n",
    " 'high school diploma',\n",
    " 'associate\\'s degree',\n",
    " 'bachelor\\'s degree',\n",
    " 'master\\'s degree',\n",
    " 'doctorate\\'s degree']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "631fe0f3-9479-4490-b091-a31839b9fd5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Randomly Select a Job and an Interested User\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879919b9-bbcc-4260-ade5-37b9f1f845bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company: northwestern mutual\njob: Recruiter\nNumber of employees in the company: 576\n"
     ]
    }
   ],
   "source": [
    "job_posting = company_features_jobs.orderBy(rand(seed=2)).limit(1)\n",
    "company_name = job_posting.select('name').collect()[0][0]\n",
    "job_name = job_posting.select('job').collect()[0][0]\n",
    "\n",
    "print(\"company:\", company_name)\n",
    "print(\"job:\", job_name)\n",
    "print(\"Number of employees in the company:\", job_posting.select('num_emp').collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef90f434-e063-4fda-80f9-88dc3c8cadfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID: susan-chon-b32638123\nUser features:\nyears of education: 11.0\nyears of experience: 14.666666666666666\nrecommendations: 0.0\nvolunteer experience: 0.0\nenglish: 1.0\nspanish: 0.0\nchinese: 0.0\ntagalog: 0.0\nvietnamese: 0.0\nfrench: 0.0\nkorean: 1.0\ngerman: 0.0\narabic: 0.0\nrussian: 0.0\nitalian: 0.0\nportuguese: 0.0\npolish: 0.0\nhindi: 0.0\njapanese: 0.0\nurdu: 0.0\ngujarati: 0.0\npersian: 0.0\ntelugu: 0.0\ntamil: 0.0\ngreek: 0.0\narmenian: 0.0\nhaitian creole: 0.0\nfilipino: 0.0\nbengali: 0.0\npanjabi: 0.0\nkannada: 0.0\nmalayalam: 0.0\nmarathi: 0.0\nodia: 0.0\nsindhi: 0.0\nsinhala: 0.0\nassamese: 0.0\nnepali: 0.0\nharyanvi: 0.0\nrajasthani: 0.0\nchhattisgarhi: 0.0\nmaithili: 0.0\nbhojpuri: 0.0\nmagahi: 0.0\nkashmiri: 0.0\nhebrew: 0.0\nhigh school diploma: 1.0\nassociate's degree: 1.0\nbachelor's degree: 1.0\nmaster's degree: 1.0\ndoctorate's degree: 1.0\n"
     ]
    }
   ],
   "source": [
    "user = user_feature_vectors.orderBy(rand(seed=22)).limit(1)\n",
    "user_features = user.select('features').collect()[0][0].toArray().tolist()\n",
    "user_id = user.select('id').collect()[0][0]\n",
    "\n",
    "print(\"User ID:\", user_id)\n",
    "print(\"User features:\")\n",
    "for i in range(len(user_features)):\n",
    "    print(f\"{ordered_features[i]}: {user_features[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54439da-2491-41aa-be0f-f5928de1e1b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Give Initial Feedback to User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97bac97-c7cc-4a35-80a9-017eb890e19c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define cosine similarity function\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(x * y for x, y in zip(vector1, vector2))\n",
    "    magnitude_vector1 = math.sqrt(sum(x ** 2 for x in vector1))\n",
    "    magnitude_vector2 = math.sqrt(sum(y ** 2 for y in vector2))\n",
    "    return 0 if magnitude_vector1 == 0 or magnitude_vector2 == 0 else dot_product / (magnitude_vector1 * magnitude_vector2)\n",
    "\n",
    "def find_insert_position(sorted_list, new_number):\n",
    "    # Iterate through the sorted list\n",
    "    for i, num in enumerate(sorted_list):\n",
    "        # If the new number is smaller or equal to the current number, return the index\n",
    "        if new_number <= num:\n",
    "            return i\n",
    "    # If the new number is greater than all numbers in the list, return the length of the list\n",
    "    return len(sorted_list)\n",
    "\n",
    "def get_rank(user_features, user_company):\n",
    "    user_similarity = cosine_similarity(user_features, user_company[MEAN_VEC])\n",
    "    percentage_rank = find_insert_position(user_company[SORTED_SIMILARITIES], user_similarity) / len(user_company[SORTED_SIMILARITIES])\n",
    "    return percentage_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09c65ff7-d12d-4bdf-a39b-323c86eea8db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "NUM_EMP = 2\n",
    "THRESHOLD = 0 # Change as desired\n",
    "MEAN_VEC = 1\n",
    "SORTED_SIMILARITIES = 3\n",
    "\n",
    "def calc_similarity_score(user_company, user_features):\n",
    "\n",
    "    user_company = company_features.filter(F.col('current_company_name') == user_company).first()\n",
    "\n",
    "    # Assume we have in our data all companies we allow access to this feature from.  \n",
    "    rank = get_rank(user_features, user_company)\n",
    "    mean_vec = user_company[MEAN_VEC]\n",
    "\n",
    "### Since we are currently working with select companies, we will not use the similarity algorithm we developed in this example. However, we assume when our feature is incorporated in LinkedIn we will have the necessary additional resources to amend this. We can find a similar company like we do later to find questions, but we prefer not to duplicate the code as it's function is identical. \n",
    "\n",
    "    if user_company[NUM_EMP] < THRESHOLD:\n",
    "\n",
    "        # Among companies with at least THRESHOLD employees?\n",
    "        similar_company, comp_similarity_score = None \n",
    "        sim_comp_rank = get_rank(user_features, similar_company)\n",
    "        weight = (similar_company[NUM_EMP] / (similar_company[NUM_EMP] + user_company[NUM_EMP]))*comp_similarity_score\n",
    "        rank = weight*sim_comp_rank + (1 - weight)*rank\n",
    "\n",
    "        mean_vec = weight * similar_company[MEAN_VEC] + (1 - weight)*mean_vec\n",
    "\n",
    "    return rank*100, mean_vec\n",
    "\n",
    "def prepare_message(rank, mean_vec, user_features):\n",
    "\n",
    "    msg = f'Your perliminary calculated chances of suitability for this job are {rank:.1f}%. \\n\\n'\n",
    "\n",
    "    good_msg = ''\n",
    "    bad_msg = ''\n",
    "\n",
    "    good_feats = []\n",
    "    bad_feats = []    \n",
    "    for i in range(3):\n",
    "        feat = user_features[i]\n",
    "        mean_feat = mean_vec[i]\n",
    "        if feat >= mean_feat:\n",
    "            good_feats.append(ordered_features[i])\n",
    "        else:\n",
    "            bad_feats.append((ordered_features[i], math.ceil(mean_feat - feat)))\n",
    "\n",
    "    if len(good_feats):\n",
    "        good_msg += f'You have matched or exceeded the recommended amount of {\", \".join(good_feats)} for this job. '\n",
    "\n",
    "    if len (bad_feats):\n",
    "        bad_msg += f'You have {\", \".join([str(f[1]) + \" less \" + f[0] for f in bad_feats])} than recommended for this job. '\n",
    "\n",
    "    if mean_vec[3] > 0.2 and not user_features[3]:\n",
    "        bad_msg += 'You have not reported any volunteer experience, and it is recommended to have for this job. '\n",
    "\n",
    "    elif mean_vec[3] > 0.2 and user_features[3]:\n",
    "        good_msg += 'You have reported having volunteer experience, which is recommended to have for this job. '\n",
    "\n",
    "    langs = np.where(np.array(mean_vec)[4:-5] >= 0.2)[0]\n",
    "    good_langs = []\n",
    "    bad_langs = []\n",
    "\n",
    "    for lang in langs:\n",
    "        if user_features[4:-5][lang]:\n",
    "            good_langs.append(ordered_features[4:-5][lang])\n",
    "        else:\n",
    "            bad_langs.append(ordered_features[4:-5][lang])\n",
    "\n",
    "    if good_langs:\n",
    "        good_msg += f'You have proficiency in the recommended languages: {\", \".join(good_langs)}. '\n",
    "\n",
    "    if bad_langs:\n",
    "        bad_msg += f'You lack proficiency in the recommended languages: {\", \".join(bad_langs)}. '\n",
    "\n",
    "    for mean_deg in [-1, -2, -3, -4, -5]:\n",
    "        if mean_vec[mean_deg] >= 0.5:\n",
    "            break\n",
    "\n",
    "    for user_deg in [-1, -2, -3, -4, -5]:\n",
    "        if mean_vec[user_deg] >= 0.5:\n",
    "            break\n",
    "\n",
    "    if user_deg >= mean_deg:\n",
    "        good_msg += f'The recommended education level is a {ordered_features[mean_deg]}, which you achieved. '\n",
    "\n",
    "    else:\n",
    "        bad_msg += f'The recommended education level is a {ordered_features[mean_deg]}, and you have a {ordered_features[user_deg]}. '\n",
    "\n",
    "    msg += good_msg + '\\n\\n' + bad_msg \n",
    "    msg_dict = {'good': good_msg,\n",
    "                'bad': bad_msg}\n",
    "\n",
    "    return msg, msg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5308cbe8-56e0-40d8-bb12-7d8e56dadfde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_info(user_features, user_company):\n",
    "\n",
    "    user_score, mean_vec = calc_similarity_score(user_company, user_features)\n",
    "    msg, msg_dict = prepare_message(user_score, mean_vec, user_features)\n",
    "\n",
    "    return msg, msg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b845c9-8d4c-429e-92d9-c3d98492ae0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "Your perliminary calculated chances of suitability for this job are 63.2%. \n",
       "\n",
       "You have matched or exceeded the recommended amount of years of education, years of experience for this job. The recommended education level is a bachelor's degree, which you achieved. \n",
       "\n",
       "You have 1 less recommendations than recommended for this job. "
      ],
      "text/plain": [
       "Out[9]: ",
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "msg, msg_dict = get_info(user_features, company_name)\n",
    "Markdown(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e71c60-3e47-46ac-ba2e-41c0e5560a73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feed Data To Generative Model to Begin Interview Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809d9728-2190-4b8e-a1f8-a8315587e999",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Configure Gemini Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb2736a-a8b5-4f81-969d-880358eb2d7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6f89b3-a68f-4c2c-b747-46ff24d556ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "  \n",
    "api_key = 'AIzaSyAED20eopE-GdR02Sn0rnPTWootmKilOpM'\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af22e2b0-eb81-4dd5-bf69-336efe27aa07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "INPUT_TOKEN_LIMIT = 30720"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b8435ba-e347-444c-90f4-dbfb170d7b42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Find Relevant Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2a9ca69-e6b7-4c7e-8d02-cae49e8c8a2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------------------+\n|     data_name|category|            question|\n+--------------+--------+--------------------+\n|American Water|    null|1. The utilities ...|\n|American Water|    null|2. Our industry i...|\n|American Water|    null|3. Have you perso...|\n|American Water|    null|4. American Water...|\n|American Water|    null|5. Why do you wan...|\n|American Water|    null|6. What are your ...|\n|American Water|    null|7. At American Wa...|\n|American Water|    null|8. Why do you wan...|\n|American Water|    null|9. Tell me about ...|\n|American Water|    null|10. If we hire yo...|\n+--------------+--------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Scraped question data\n",
    "questions_df = spark.read \\\n",
    "    .csv(\"/FileStore/shared_uploads/naomi.derel@campus.technion.ac.il/questions.csv\", header=True)\n",
    "questions_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a496ea-b62a-4677-98cd-e4010083a44c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# # Define data preprocessing functions\n",
    "# def clean_text(text):\n",
    "#     # Convert text to lowercase\n",
    "#     text = text.lower()\n",
    "#     # Remove punctuation\n",
    "#     text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#     return text\n",
    "\n",
    "# # Register UDF for data preprocessing\n",
    "# clean_text_udf = udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9689d337-cad4-49a5-a551-5e205009977f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "\u001B[0;32m<command-3791678015591082>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[0;31m# find records for informative companies:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 5\u001B[0;31m \u001B[0minformative_companies_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mquestions_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclean_text_udf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'data_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistinct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0minformative_companies_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompanies\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'clean_name'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclean_text_udf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minformative_companies_names\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'clean_name'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0minformative_companies_names\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"semi\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'col' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-3791678015591082>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# find records for informative companies:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0minformative_companies_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mquestions_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclean_text_udf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'data_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malias\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistinct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0minformative_companies_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompanies\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'clean_name'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclean_text_udf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minformative_companies_names\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'clean_name'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0minformative_companies_names\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"semi\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mNameError\u001B[0m: name 'col' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'col' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Load companies df\n",
    "# companies = spark.read.parquet('/linkedin/companies')\n",
    "\n",
    "# # find records for informative companies:\n",
    "# informative_companies_names = questions_df.select(clean_text_udf(F.col('data_name')).alias('name')).distinct()\n",
    "# informative_companies_df = companies.withColumn('clean_name', clean_text_udf(F.col('name'))) \\\n",
    "#     .join(informative_companies_names, F.col('clean_name') == informative_companies_names.name, \"semi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06480a8d-4029-4348-8b9b-12fa2893bd64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n+-------------------+\n|               name|\n+-------------------+\n|northwestern mutual|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "informative_companies_df = spark.read.parquet(\"/FileStore/shared_uploads/naomi.derel@campus.technion.ac.il/companies_have_qs.parquet\")\n",
    "\n",
    "# find record for current company:\n",
    "my_company = company_features_jobs.filter(col(\"name\") == company_name).limit(1)\n",
    "print(my_company.count())\n",
    "my_company.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e57fe2-2dd3-4385-a510-1201c166c445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[40]: 28"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "similarity_features_cols_short = ['name', 'about', 'company_size','slogan']\n",
    "similarity_features_cols = ['company_A_name', 'company_B_name', 'company_A_about', 'company_B_about', 'company_A_company_size', 'company_B_company_size', 'company_A_slogan', 'company_B_slogan']\n",
    "\n",
    "df_alias = informative_companies_df.alias(\"df\")\n",
    "single_row_alias = my_company.alias(\"single_row\")\n",
    "\n",
    "for col_name in similarity_features_cols_short:\n",
    "    single_row_alias = single_row_alias.withColumnRenamed(col_name, \"company_A_\" + col_name)\n",
    "    df_alias = df_alias.withColumnRenamed(col_name, \"company_B_\" + col_name)\n",
    "df_alias = df_alias.withColumnRenamed('url', \"company_B_url\")\n",
    "\n",
    "# Cross join the single row with the entire DataFrame\n",
    "combined_df = single_row_alias.crossJoin(df_alias)\n",
    "combined_df = combined_df.select(*similarity_features_cols, 'company_B_url')\n",
    "\n",
    "# Show the result\n",
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0943cea5-4c97-48ee-8f53-8febd8aacb89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLP imports:\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Pyspark imports\n",
    "from pyspark.sql.functions import udf, abs, size, col, when, concat, array, lit, rand, count, explode, regexp_replace, expr\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# download NLTK stopwords:\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nltk.download('punkt')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# define a function for stemming:\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and remove stopwords.\n",
    "    input: text to tokenize\n",
    "    output: preprocessed text\n",
    "    \"\"\"\n",
    "    tokens = [word for word in word_tokenize(text.lower()) if word.isalpha() and word not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    stemmed_text = ' '.join(stemmed_tokens)\n",
    "    return stemmed_text\n",
    "\n",
    "# define a function to calculate similarity between 2 text blocks:\n",
    "def calculate_similarity(text1, text2, stemming=True):\n",
    "    \"\"\"\n",
    "    Find cosine similarity between tf-idf vectors of the input texts.\n",
    "    ouput: single similarity score\n",
    "    \"\"\"\n",
    "    # if one or more of the texts are empty, define as 0 similarity:\n",
    "    if text1 is None or text2 is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # preprocess the texts if neccesary: (not good for short imputs like names)\n",
    "    if stemming:\n",
    "        text1 = stem_text(text1)\n",
    "        text2 = stem_text(text2)\n",
    "\n",
    "    # create TF-IDF vectors:\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([text1, text2])\n",
    "\n",
    "    # calculate cosine similarity:\n",
    "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "    return float(similarity_score)\n",
    "\n",
    "# udf for text similarity:\n",
    "similarity_udf = udf(lambda text1, text2, stemming: calculate_similarity(text1, text2, stemming), DoubleType())\n",
    "\n",
    "### Create general features:\n",
    "\n",
    "def extract_small_features_from_data(data, train):\n",
    "    \"\"\"\n",
    "    Assume data contains the columns in similarity_features_cols.\n",
    "    Create a features df for random forest model\n",
    "    \"\"\"\n",
    "\n",
    "    # about (with stemming):\n",
    "    data = data.withColumn('about_similarity_score', similarity_udf(col('company_A_about'), col('company_B_about'), lit(True)))\n",
    "\n",
    "    # name (without stemming):\n",
    "    data = data.withColumn('name_similarity_score', similarity_udf(data['company_A_name'], data['company_B_name'], lit(False)))\n",
    "\n",
    "    # slogan (without stemming):\n",
    "    data = data.withColumn('slogan_similarity_score', similarity_udf(data['company_A_slogan'], data['company_B_slogan'], lit(False)))\n",
    "\n",
    "    # interval numerical variable for compay size:\n",
    "    comp_size_dict = {\n",
    "        '': 0,\n",
    "        'None': 0,\n",
    "        '1 employee': 1,\n",
    "        '2-10 employees': 2,\n",
    "        '11-50 employees': 3,\n",
    "        '51-200 employees': 4,\n",
    "        '201-500 employees': 5,\n",
    "        '501-1,000 employees': 6,\n",
    "        '1,001-5,000 employees': 7,\n",
    "        '5,001-10,000 employees': 8,\n",
    "        '10,001+ employees': 9\n",
    "    }\n",
    "    comp_size_udf = udf(lambda x: comp_size_dict.get(x, 0), IntegerType())\n",
    "\n",
    "    data = data.withColumn('company_A_numerical_size',\n",
    "                            when(col('company_A_company_size').isNull() | \n",
    "                                    (col('company_A_company_size') == 'None') | \n",
    "                                    (col('company_A_company_size') == ''), 0)\n",
    "                            .otherwise(comp_size_udf(col('company_A_company_size'))))\n",
    "    data = data.withColumn('company_B_numerical_size',\n",
    "                            when(col('company_B_company_size').isNull() | \n",
    "                                    (col('company_B_company_size') == 'None') | \n",
    "                                    (col('company_B_company_size') == ''), 0)\n",
    "                            .otherwise(comp_size_udf(col('company_B_company_size'))))\n",
    "\n",
    "    ## Interaction features between same variables (for better importance comparison):\n",
    "    # sizes of companies interaction:\n",
    "    data = data \\\n",
    "        .withColumn(\"size_interaction\", col(\"company_A_numerical_size\") * col(\"company_B_numerical_size\"))\n",
    "\n",
    "\n",
    "    # final features df:\n",
    "    if train:\n",
    "        final_data_features = data.select('name_similarity_score', 'about_similarity_score', 'slogan_similarity_score', 'company_A_numerical_size', 'company_B_numerical_size', 'size_interaction', 'similarity_label')\n",
    "    else:\n",
    "        final_data_features = data.select('name_similarity_score', 'about_similarity_score', 'slogan_similarity_score', 'company_A_numerical_size', 'company_B_numerical_size', 'size_interaction', 'company_B_url')\n",
    "\n",
    "    return final_data_features\n",
    "\n",
    "# UDF to extract the probability of class 1 from the vector\n",
    "def extract_class_1_probability(probability_vector):\n",
    "    return float(probability_vector[1])\n",
    "\n",
    "# Register UDF\n",
    "extract_prob = udf(extract_class_1_probability, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d8d4a8-94f3-4d0b-b4f2-57b951f7707d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(company_B_url='https://www.linkedin.com/company/honeywell2022', probability_class_1=1.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "## Load your trained Random Forest model\n",
    "model_path = \"/FileStore/shared_uploads/naomi.derel@campus.technion.ac.il/companies_model/rf_companies_similarity\"\n",
    "model = PipelineModel.load(model_path)\n",
    "\n",
    "# Extract features and include identifiers for prediction\n",
    "df = extract_small_features_from_data(combined_df, train=False)\n",
    "\n",
    "# Make predictions as before\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# Extract the probability of class 1 and include identifiers\n",
    "predictions_with_prob = predictions.withColumn(\"probability_class_1\", extract_prob(\"probability\"))\n",
    "\n",
    "# Order by probability and retain the company identifier\n",
    "max_prob_row = predictions_with_prob.orderBy(col(\"probability_class_1\").desc()).select(\"company_B_url\", \"probability_class_1\").first()\n",
    "\n",
    "print(max_prob_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eacd4f-c0d1-49b0-ab87-6f36dd4d0bad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honeywell\n"
     ]
    }
   ],
   "source": [
    "url_of_best_informative_company = max_prob_row['company_B_url']\n",
    "\n",
    "best_company_record = companies.filter(col(\"url\") == url_of_best_informative_company)\n",
    "best_company_name = best_company_record.first()['name']\n",
    "print(best_company_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26925bd6-bc42-4e2e-8534-774eaad351bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42aafdc-e473-445e-9efc-7c14b8d28d02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Northwestern Mutual is client-centric. How do you continually strengthen relationships with your exciting client base?\n2. Wealth management is an all encompassing term. How would you explain what we do at Northwestern Mutual to someone unfamiliar with our company?\n3. As a wealth management professional, have you ever been asked to act unethically? If so, what did you do?\n4. In an initial client meeting, how do you first assess a client's financial position?\n5. How do you stay current on developments and trends in wealth management?\n6. Walk me through your education and how it relates to a career in wealth management.\n7. How would you explain mutual funds to someone unfamiliar with the concept?\n8. This role with Northwestern Mutual is very independent. How do you prospect for new customers?\n9. Tell me about the systems and tools that you lean on to do your job.\n10. Do you prefer to work independently, or with partners?\n11. Do you have your Certified Financial Planner (CFP) designation? If not, is this something you would like to achieve?\n12. As a Financial Advisor, your compensation is tied directly to your productivity. How do you feel about working in a highly commissioned role?\n13. Analytical skills are critical to success as a Financial Advisor. In which ways are you analytically minded?\n14. How do you show potential clients that you are a trusted source for information on wealth management and other financial topics?\n15. Which factors do you believe most impact the growth of the wealth management industry?\n16. Financial Advisors with Northwestern Mutual who are active in their community see more success. Tell me about an organization or group outside of work with which you are involved.\n17. Many Northwestern Mutual employees work as though they are independent business owners. Talk to me about what makes your entrepreneurial by nature.\n18. When you suffer a setback in your portfolio, how does that emotionally affect you?\n19. How would you build a relationship with a client with an intimidating nature?\n20. Why do you want to work in wealth management, and where would you like your career to take you?\n21. Do you have experience preparing and delivering presentations?\n22. How will your skills complement Northwestern Mutual, if hired?\n23. What skills did you learn in your current role that will help you succeed at Northwestern Mutual?\n24. How do you deal with client rejection or objections?\n25. Name for me what you believe to be the biggest challenge facing the wealth management industry today.\n26. Would you cold call for a year if it meant you had a steady client list afterward?\n27. Have you ever broken a non-disclosure, privacy, or confidentiality agreement?\n28. Do you have a personal or professional mission statement?\n29. Do you have any questions about this job, or working for Northwestern Mutual?\n30. What are your pay expectations for this role?\n"
     ]
    }
   ],
   "source": [
    "if informative_companies_df.filter(lower(col('name')) == company_name.lower()).count() > 0:\n",
    "    similar_questions_df = questions_df.filter(lower(col('data_name')) == company_name.lower()) \\\n",
    "    .select('question')\n",
    "else:\n",
    "    similar_questions_df = questions_df.filter(lower(col('data_name')) == best_company_name.lower()) \\\n",
    "        .select('question')\n",
    "\n",
    "similar_questions = [row['question'] for row in similar_questions_df.collect()]\n",
    "similar_questions = \"\\n\".join(q for q in similar_questions)\n",
    "print(similar_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f299fc2-2912-40d3-a89b-cc764ca4a4d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### example user analysis (format can be changed to anything as long as it is also changed in the prompt - next cell)\n",
    "\n",
    "user_analysis = msg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c5e37b-802c-4022-9d75-d3754f2ebe6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretend you are an interviewer at northwestern mutual. Ask me some questions in a job interview for a Recruiter position. Output the questions as a list.\nAsk me some questions specifically about my strengths: You have matched or exceeded the recommended amount of years of education, years of experience for this job. The recommended education level is a bachelor's degree, which you achieved. , and weaknesess: You have 1 less recommendations than recommended for this job. \nHere are some example questions from similar companies:\n1. Northwestern Mutual is client-centric. How do you continually strengthen relationships with your exciting client base?\n2. Wealth management is an all encompassing term. How would you explain what we do at Northwestern Mutual to someone unfamiliar with our company?\n3. As a wealth management professional, have you ever been asked to act unethically? If so, what did you do?\n4. In an initial client meeting, how do you first assess a client's financial position?\n5. How do you stay current on developments and trends in wealth management?\n6. Walk me through your education and how it relates to a career in wealth management.\n7. How would you explain mutual funds to someone unfamiliar with the concept?\n8. This role with Northwestern Mutual is very independent. How do you prospect for new customers?\n9. Tell me about the systems and tools that you lean on to do your job.\n10. Do you prefer to work independently, or with partners?\n11. Do you have your Certified Financial Planner (CFP) designation? If not, is this something you would like to achieve?\n12. As a Financial Advisor, your compensation is tied directly to your productivity. How do you feel about working in a highly commissioned role?\n13. Analytical skills are critical to success as a Financial Advisor. In which ways are you analytically minded?\n14. How do you show potential clients that you are a trusted source for information on wealth management and other financial topics?\n15. Which factors do you believe most impact the growth of the wealth management industry?\n16. Financial Advisors with Northwestern Mutual who are active in their community see more success. Tell me about an organization or group outside of work with which you are involved.\n17. Many Northwestern Mutual employees work as though they are independent business owners. Talk to me about what makes your entrepreneurial by nature.\n18. When you suffer a setback in your portfolio, how does that emotionally affect you?\n19. How would you build a relationship with a client with an intimidating nature?\n20. Why do you want to work in wealth management, and where would you like your career to take you?\n21. Do you have experience preparing and delivering presentations?\n22. How will your skills complement Northwestern Mutual, if hired?\n23. What skills did you learn in your current role that will help you succeed at Northwestern Mutual?\n24. How do you deal with client rejection or objections?\n25. Name for me what you believe to be the biggest challenge facing the wealth management industry today.\n26. Would you cold call for a year if it meant you had a steady client list afterward?\n27. Have you ever broken a non-disclosure, privacy, or confidentiality agreement?\n28. Do you have a personal or professional mission statement?\n29. Do you have any questions about this job, or working for Northwestern Mutual?\n30. What are your pay expectations for this role?\n"
     ]
    }
   ],
   "source": [
    "# Create the prompt:\n",
    "\n",
    "initial_prompt = f\"Pretend you are an interviewer at {company_name}. Ask me some questions in a job interview for a {job_name} position. Output the questions as a list.\"\n",
    "\n",
    "if user_analysis is not None:\n",
    "    initial_prompt += f\"\\nAsk me some questions specifically about my strengths: {user_analysis['good']}, and weaknesess: {user_analysis['bad']}\"\n",
    "\n",
    "if len(similar_questions) > 0:\n",
    "    initial_prompt += f\"\\nHere are some example questions from similar companies:\\n{similar_questions}\"\n",
    "else:\n",
    "    initial_prompt += f\"\\nHere are some examples for interview questions:\\n{example_questions}\"\n",
    "\n",
    "print(initial_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1264cd52-609b-4e33-9c22-51b4b4aa35fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "> **Strengths Questions:**\n",
       "> \n",
       "> * Describe a time when you successfully exceeded expectations in a recruiting role and the impact it had on your organization.\n",
       "> * How have your previous experiences prepared you for the high-volume recruiting environment at Northwestern Mutual?\n",
       "> * What are your strengths that you believe would make you a valuable asset to our team?\n",
       "> \n",
       "> **Weakness Questions:**\n",
       "> \n",
       "> * You have one less recommendation than recommended for this job. How do you plan to address this potential shortcoming?\n",
       "> * What specific areas do you believe you need to improve in order to meet the expectations of this role?\n",
       "> \n",
       "> **Additional Questions:**\n",
       "> \n",
       "> * How do you stay up-to-date on industry best practices and trends in recruiting?\n",
       "> * What strategies do you use to identify and attract top-tier candidates?\n",
       "> * How do you handle the challenges of managing multiple recruiting pipelines simultaneously?\n",
       "> * How do you assess the fit between a candidate and a specific position?\n",
       "> * What is your approach to building and maintaining relationships with hiring managers and stakeholders?\n",
       "> * How do you evaluate the success of your recruiting efforts?\n",
       "> * What is your understanding of Northwestern Mutual's culture and values?\n",
       "> * Why are you interested in working for Northwestern Mutual specifically?\n",
       "> * What are your long-term career aspirations and how does this role align with them?"
      ],
      "text/plain": [
       "Out[47]: ",
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initial_prompt:\n",
    "response = gemini_model.generate_content(initial_prompt)\n",
    "\n",
    "# generate answers:\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1391114f-9929-414e-b4ac-3cd5034ba78d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**now we pretend that the user pressed some button and wants to simulate an interview.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad73c21-c6f6-4526-8351-8981e3d38b05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# assume that the user clicked a question to answer:\n",
    "question_picked = \"Describe a time when you successfully exceeded expectations in a recruiting role and the impact it had on your organization.\"\n",
    "\n",
    "interview_prompt_instruction = f\"Pretend you are an interviewer, and you asked me the question: {question_picked}. Respond to my answer and give me an evaluation about my answer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efba13f-6489-4485-a5d6-d669003173e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "> **Interviewer's Response:**\n",
       "> \n",
       "> Thank you for your thoughtful answer. I appreciate your enthusiasm and alignment with Amazon's culture of innovation. Your passion for pushing boundaries and driving change is evident in your response.\n",
       "> \n",
       "> Specifically, your mention of leading cross-functional teams in developing and implementing cutting-edge technologies demonstrates your ability to collaborate effectively and drive results. Your commitment to operational efficiency and enhancing customer experiences also aligns with Amazon's priorities.\n",
       "> \n",
       "> **Evaluation:**\n",
       "> \n",
       "> Overall, your answer successfully exceeded expectations. You showcased:\n",
       "> \n",
       "> * A clear understanding of Amazon's values and culture\n",
       "> * A track record of innovation and driving change\n",
       "> * A commitment to continuous learning and growth\n",
       "> * A proactive approach to technology advancements\n",
       "> * A strong alignment between your skills and Amazon's business objectives\n",
       "> \n",
       "> Your response is well-structured and provides specific examples that demonstrate your capabilities. You have effectively highlighted how your experience and mindset would contribute to Amazon's culture of innovation and transformative impact.\n",
       "> \n",
       "> **Next Steps:**\n",
       "> \n",
       "> I would like to explore further with you how you have quantified the impact of your innovation efforts in previous roles. Can you provide specific metrics or case studies that demonstrate the tangible benefits and outcomes you have achieved?"
      ],
      "text/plain": [
       "Out[64]: ",
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### generate a theoretical user response with chatGPT4 as an example\n",
    "\n",
    "# good example:\n",
    "good_user_input = \"In my previous role as a recruiting manager, I encountered a situation where our company urgently needed to fill several key positions within a short timeframe due to an unexpected surge in project demand. Recognizing the criticality of the situation, I immediately devised a strategic recruitment plan that involved leveraging multiple channels, including job boards, social media platforms, and professional networks, to source top-tier candidates. Additionally, I implemented streamlined screening and interview processes to expedite the hiring process without compromising quality. Through meticulous candidate assessment and proactive engagement, I successfully identified and recruited a highly talented pool of candidates within the specified timeframe, exceeding the hiring targets by 20%. This influx of skilled professionals significantly bolstered our team's capabilities, enabling us to deliver projects ahead of schedule and surpass client expectations. Moreover, the positive impact of this recruitment initiative extended beyond immediate project success, as it strengthened our organization's reputation as an employer of choice in the industry, attracting top talent and fostering long-term growth and success.\"\n",
    "\n",
    "response = gemini_model.generate_content(interview_prompt_instruction + user_input)\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf22170-b2d3-490f-927e-7530eaa386e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "> **Evaluation:**\n",
       "> \n",
       "> Your answer effectively addresses the question by highlighting your commitment to innovation and forward-thinking. You provide specific examples of how you have exceeded expectations in your recruiting role and the impact it has had on your organization.\n",
       "> \n",
       "> **Response:**\n",
       "> \n",
       "> \"Thank you for sharing your experience of successfully exceeding expectations in your recruiting role. Your commitment to innovation and forward-thinking is evident in your accomplishments, and I believe you would be a valuable asset to our team at Amazon.\n",
       "> \n",
       "> Your ability to identify and recruit top talent, coupled with your understanding of emerging trends and technologies, aligns perfectly with our company's ethos as pioneers. Your proactive approach and dedication to driving transformative impact would make you an exceptional addition to our team.\n",
       "> \n",
       "> I am particularly impressed with your leadership in developing and implementing cutting-edge technologies and spearheading initiatives to optimize operational efficiency. Your ability to think strategically and drive change aligns with our company's culture of innovation and growth.\n",
       "> \n",
       "> I am confident that your skills and experience would enable you to make a significant contribution to Amazon. Your passion for exploration and dedication to excellence are qualities that we highly value in our team members.\n",
       "> \n",
       "> Thank you again for sharing your experience. We would like to invite you to the next round of interviews to further explore your qualifications and how you can bring your innovative mindset to our organization.\""
      ],
      "text/plain": [
       "Out[65]: ",
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bad example:\n",
    "bad_user_input = \"Yeah, so there was this one time when I was doing recruiting stuff, and, like, we needed to hire some people real quick because, you know, we had a lot of work piling up and not enough hands to do it. So, I kinda just did what I usually do, like posting job ads online and stuff. I didn't really have a plan or anything, just kinda winged it. Anyway, we managed to hire a few folks eventually, but it took longer than expected, and we had to settle for some candidates who weren't exactly what we were looking for. It was kind of a mess, to be honest, and it didn't really have much of an impact on the organization. We got the work done eventually, but it wasn't anything to write home about.\"\n",
    "\n",
    "response = gemini_model.generate_content(interview_prompt_instruction + user_input)\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bde694-9a38-4f19-bed5-34ec17dcfaa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Sentiment Analysis Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05cb42ec-4aee-4032-a5f0-a37032dcce53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334b8809-86a4-43a4-be1f-cf0dd98990a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nOut[67]: <All keys matched successfully>"
     ]
    }
   ],
   "source": [
    "model_weights_path = '/dbfs/FileStore/shared_uploads/naomi.derel@campus.technion.ac.il/sent_analys_model/model_weights.pth'\n",
    "\n",
    "loaded_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2).to(torch.device('cpu'))\n",
    "loaded_model.load_state_dict(torch.load(model_weights_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1783ce-401e-4a35-90f7-283edb298799",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def inference_of_answer(model, text):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    if torch.backends.mps.is_available():\n",
    "        model = model.to(torch.device('mps'))\n",
    "        input_ids = to_cuda(input_ids)\n",
    "        attention_mask = to_cuda(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    return [probs[0][0].item(), probs[0][1].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4510faaf-67ed-48a7-95ce-b29cb390da32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good: 6.348652095766738e-05, Bad: 0.9999364614486694\nDecision: Bad\n"
     ]
    }
   ],
   "source": [
    "# bad example:\n",
    "prob_bad, prob_good = inference_of_answer(loaded_model, bad_user_input)\n",
    "\n",
    "print(f\"Good: {prob_good}, Bad: {prob_bad}\")\n",
    "print(\"Decision: \" + \"Good\" if prob_good > prob_bad else \"Decision: \" + \"Bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a42764-b8dd-4274-914b-96f1f5be3d47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good: 0.9999399185180664, Bad: 6.013096935930662e-05\nDecision: Good\n"
     ]
    }
   ],
   "source": [
    "# good example:\n",
    "prob_bad, prob_good = inference_of_answer(loaded_model, good_user_input)\n",
    "\n",
    "print(f\"Good: {prob_good}, Bad: {prob_bad}\")\n",
    "print(\"Decision: \" + \"Good\" if prob_good > prob_bad else \"Decision: \" + \"Bad\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project - Example",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
